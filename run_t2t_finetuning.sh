TOKENIZERS_PARALLELISM=true CUDA_VISIBLE_DEVICES=0,1,2,3 python run_t2t_finetuning.py \
    --model_name_or_path bigscience/bloomz-560m \
    --do_train \
    --num_train_epochs 5 \
    --logging_strategy steps \
    --logging_steps 10 \
    --evaluation_strategy epoch \
    --eval_steps 1 \
    --save_strategy epoch \
    --save_steps 1 \
    --save_total_limit 5 \
    --output_dir ./save/monolingual/bloomz-560m \
    --learning_rate 1e-5 \
    --preprocessing_num_workers 16 \
    --dataloader_num_workers 16 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --overwrite_output_dir \
    --augmentation_type monolingual \
    --fp16

TOKENIZERS_PARALLELISM=true CUDA_VISIBLE_DEVICES=4,5,6,7 python run_t2t_finetuning.py \
    --model_name_or_path bigscience/bloomz-560m \
    --do_train \
    --num_train_epochs 5 \
    --logging_strategy steps \
    --logging_steps 10 \
    --evaluation_strategy epoch \
    --eval_steps 1 \
    --save_strategy epoch \
    --save_steps 1 \
    --save_total_limit 5 \
    --output_dir ./save/translation/bloomz-560m \
    --learning_rate 1e-5 \
    --preprocessing_num_workers 16 \
    --dataloader_num_workers 16 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 16 \
    --overwrite_output_dir \
    --augmentation_type translation \
    --fp16

TOKENIZERS_PARALLELISM=true CUDA_VISIBLE_DEVICES=8,9 python run_t2t_finetuning.py \
    --model_name_or_path bigscience/bloomz-560m \
    --do_train \
    --num_train_epochs 5 \
    --logging_strategy steps \
    --logging_steps 10 \
    --evaluation_strategy epoch \
    --eval_steps 1 \
    --save_strategy epoch \
    --save_steps 1 \
    --save_total_limit 5 \
    --output_dir ./save/bilingual/bloomz-560m \
    --learning_rate 1e-5 \
    --preprocessing_num_workers 16 \
    --dataloader_num_workers 16 \
    --per_device_train_batch_size 32 \
    --per_device_eval_batch_size 32 \
    --overwrite_output_dir \
    --augmentation_type bilingual \
    --fp16
